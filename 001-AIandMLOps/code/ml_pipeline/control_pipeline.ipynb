{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ff3d1b",
   "metadata": {},
   "source": [
    "### Helper function to compile and run the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_job(p_name, p_func):\n",
    "    from datetime import datetime\n",
    "    from kfp.v2 import compiler\n",
    "    from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    PROJECT_ID = 'cloud-sandbox-danielw'\n",
    "    BUCKET = 'gs://ai-demo-uscentral'\n",
    "    REGION = \"us-central1\"\n",
    "\n",
    "    PIPELINE_DEF = p_name + \".json\"\n",
    "    PIPELINE_ROOT = \"{bucket}/pipeline_root/{pipeline}/{timestamp}\".format(\n",
    "        bucket = BUCKET, \n",
    "        pipeline = p_name, \n",
    "        timestamp = TIMESTAMP\n",
    "    )\n",
    "\n",
    "    # compile the pipeline\n",
    "    compiler.Compiler().compile(\n",
    "        pipeline_func = p_func, \n",
    "        package_path = PIPELINE_DEF\n",
    "    )\n",
    "\n",
    "    # connect to Vertex AI platform\n",
    "    api_client = AIPlatformClient(\n",
    "        project_id = PROJECT_ID,\n",
    "        region = REGION,\n",
    "    )\n",
    "\n",
    "    # submit the pipeline\n",
    "    response = api_client.create_run_from_job_spec(\n",
    "        job_spec_path = PIPELINE_DEF, \n",
    "        pipeline_root = PIPELINE_ROOT\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450072df",
   "metadata": {},
   "source": [
    "### Conditional execution\n",
    "You can use the `with dsl.Condition(task1.outputs[\"output_name\"] = \"value\"):` context to execute parts of the pipeline conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op, InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def get_random_int_op(minimum: int, maximum: int) -> int:\n",
    "    \"\"\"Generate a random number between minimum and maximum (inclusive).\"\"\"\n",
    "    import random\n",
    "    result = random.randint(minimum, maximum)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "@func_to_container_op\n",
    "def flip_coin_op() -> str:\n",
    "    \"\"\"Flip a coin and output heads or tails randomly.\"\"\"\n",
    "    import random\n",
    "    result = random.choice(['heads', 'tails'])\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "@func_to_container_op\n",
    "def print_op(message: str):\n",
    "    \"\"\"Print a message.\"\"\"\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='conditional-pipeline',\n",
    "    description='Shows how to use dsl.Condition().'\n",
    ")\n",
    "def conditional_pipeline():\n",
    "    flip = flip_coin_op()\n",
    "    with dsl.Condition(flip.output == 'heads', name = \"heads\"):\n",
    "        random_num_head = get_random_int_op(0, 9)\n",
    "        with dsl.Condition(random_num_head.output > 5):\n",
    "            print_op('heads and %s > 5!' % random_num_head.output)\n",
    "        with dsl.Condition(random_num_head.output <= 5):\n",
    "            print_op('heads and %s <= 5!' % random_num_head.output)\n",
    "\n",
    "    with dsl.Condition(flip.output == 'tails', name = \"tails\"):\n",
    "        random_num_tail = get_random_int_op(10, 19)\n",
    "        with dsl.Condition(random_num_tail.output > 15):\n",
    "            print_op('tails and %s > 15!' % random_num_tail.output)\n",
    "        with dsl.Condition(random_num_tail.output <= 15):\n",
    "            print_op('tails and %s <= 15!' % random_num_tail.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f75e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"conditional_pipeline\", conditional_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaa8c3",
   "metadata": {},
   "source": [
    "### Exit handlers\n",
    "You can use `with dsl.ExitHandler(exit_task):` context to execute a task when the rest of the pipeline finishes (succeeds or fails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad510b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def fail_op(message: str):\n",
    "    \"\"\"Fails.\"\"\"\n",
    "    import sys\n",
    "    print(message)    \n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='exit-handler-pipeline',\n",
    "    description='Shows how to use dsl.Condition() and dsl.ExitHandler().'\n",
    ")\n",
    "def exit_handler_pipeline():\n",
    "    exit_task = print_op('Exit handler has worked!')\n",
    "    \n",
    "    with dsl.ExitHandler(exit_task):\n",
    "        flip = flip_coin_op()\n",
    "        \n",
    "        with dsl.Condition(flip.output == 'heads'):\n",
    "            fail_op(\"Failing the run to demonstrate that exit handler still gets executed.\")\n",
    "\n",
    "        with dsl.Condition(flip.output == 'tails'):\n",
    "            fail_op(\"Failing the run to demonstrate that exit handler still gets executed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"exit_handler_pipeline\", exit_handler_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5108a4b",
   "metadata": {},
   "source": [
    "### Loop\n",
    "You can use `with dsl.ParallelFor():` context to loop through a list or array and execute tasks in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "\n",
    "@components.create_component_from_func\n",
    "def args_generator_op() -> str:\n",
    "    return '[1.1, 1.2, 1.3]'\n",
    "\n",
    "\n",
    "@components.create_component_from_func\n",
    "def print_op(s: float):\n",
    "    print(s)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='pipeline-with-loop-output')\n",
    "def loop_pipeline():\n",
    "    args_generator = args_generator_op()\n",
    "    with dsl.ParallelFor(args_generator.output) as item:\n",
    "        print_op(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7634b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"loop_pipeline\", loop_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1b7b76",
   "metadata": {},
   "source": [
    "### Loop with after\n",
    "You can use `with dsl.ParallelFor():` and `task.after()` context to loop through a list or array and execute tasks in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efe5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from typing import List\n",
    "\n",
    "@components.create_component_from_func\n",
    "def print_op(text: str) -> str:\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "@components.create_component_from_func\n",
    "def concat_op(a: str, b: str) -> str:\n",
    "    print(a + b)\n",
    "    return a + b\n",
    "\n",
    "@components.create_component_from_func\n",
    "def generate_op() -> str:\n",
    "    import json\n",
    "    return json.dumps([{'a': i, 'b': i * 10} for i in range(1, 5)])\n",
    "\n",
    "@dsl.pipeline(name='pipeline-with-loop-parameter')\n",
    "def loop_after_pipeline(greeting:str='this is a test for looping through parameters'):\n",
    "    print_task = print_op(text=greeting)\n",
    "\n",
    "    generate_task = generate_op()\n",
    "    with dsl.ParallelFor(generate_task.output) as item:\n",
    "        concat_task = concat_op(a=item.a, b=item.b)\n",
    "        concat_task.after(print_task)\n",
    "        print_task_2 = print_op(concat_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abcd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"loop_after_pipeline\", loop_after_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fd84ad",
   "metadata": {},
   "source": [
    "### Loop with subgraph\n",
    "You can use `with dsl.ParallelFor():` and `with dsl.SubGraph()` context to loop through a list or array and execute tasks in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c18029",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.components.create_component_from_func\n",
    "def print_op(s: str):\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    print(s)\n",
    "\n",
    "@dsl.pipeline(name='pipeline-subgraph')\n",
    "def subgraph_pipeline():\n",
    "    loop_args = [{'A_a': 1, 'B_b': 2}, {'A_a': 10, 'B_b': 20}]\n",
    "    with dsl.SubGraph(parallelism=2):\n",
    "        with dsl.ParallelFor(loop_args) as item:\n",
    "            print_op(item)\n",
    "            print_op(item.A_a)\n",
    "            print_op(item.B_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ca828",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"subgraph_pipeline\", subgraph_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10180b2",
   "metadata": {},
   "source": [
    "### A three-step pipeline with first two running in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from typing import List\n",
    "\n",
    "@components.create_component_from_func\n",
    "def print_op(text: str) -> str:\n",
    "    print(text)\n",
    "    return text\n",
    "\n",
    "@components.create_component_from_func\n",
    "def concat_op(a: str, b: str) -> str:\n",
    "    print(a + b)\n",
    "    return a + b\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='join-pipeline',\n",
    "  description='run in parallel and prints the concatenated result.'\n",
    ")\n",
    "def join_pipeline():\n",
    "    \"\"\"A three-step pipeline with first two running in parallel.\"\"\"\n",
    "\n",
    "    print_task_1 = print_op(\"task 1\")\n",
    "    print_task_2 = print_op(\"task 2\")\n",
    "\n",
    "    concat_task = concat_op(print_task_1.output, print_task_2.output)\n",
    "    \n",
    "    print_task_3 = print_op(concat_task.output)\n",
    "    print_task_3.after(concat_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7946208",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"join_pipeline\", join_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83451009",
   "metadata": {},
   "source": [
    "### Pipeline Parallelism\n",
    "\n",
    "dsl.get_pipeline_conf().set_parallelism(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "@comp.create_component_from_func\n",
    "def print_op(msg: str):\n",
    "    \"\"\"Print a message.\"\"\"\n",
    "    print(msg)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='pipeline-parallelism',\n",
    "    description='The pipeline shows how to set the max number of parallel pods in a pipeline.'\n",
    ")\n",
    "def pipeline_parallelism():\n",
    "    op1 = print_op('hey, what are you up to?')\n",
    "    op2 = print_op('train my model.').\n",
    "    dsl.get_pipeline_conf().set_parallelism(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847dc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"pipeline_parallelism\", pipeline_parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8135a8",
   "metadata": {},
   "source": [
    "### add_pod_annotation and add_op_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace15e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "@comp.create_component_from_func\n",
    "def print_op(msg: str):\n",
    "    \"\"\"Print a message.\"\"\"\n",
    "    print(msg)\n",
    "\n",
    "def add_annotation(op):\n",
    "  op.add_pod_annotation(name='hobby', value='football')\n",
    "  return op\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='pipeline-transformer',\n",
    "    description='The pipeline shows how to apply functions to all ops in the pipeline by pipeline transformers'\n",
    ")\n",
    "def transform_pipeline():\n",
    "  op1 = print_op('hey, what are you up to?').set_display_name(\"Display Name\")\n",
    "  op2 = print_op('train my model.').add_pod_annotation(name='tag', value='print task 2')\n",
    "  # dsl.get_pipeline_conf().add_op_transformer(add_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a97ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"transform_pipeline\", transform_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95da206b",
   "metadata": {},
   "source": [
    "### Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039cf832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp\n",
    "\n",
    "@comp.create_component_from_func\n",
    "def random_failure_op(exit_codes: str):\n",
    "    \"\"\"A component that fails randomly.\"\"\"\n",
    "    import random\n",
    "    import sys\n",
    "    \n",
    "    exit_code = int(random.choice(exit_codes.split(\",\"))) \n",
    "    print(exit_code)\n",
    "    sys.exit(exit_code)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='retry-random-failures',\n",
    "    description='The pipeline includes two steps which fail randomly. It shows how to use ContainerOp(...).set_retry(...).'\n",
    ")\n",
    "def retry_pipeline():\n",
    "    op1 = random_failure_op('0,1,2,3').set_retry(10)\n",
    "    op2 = random_failure_op('0,1').set_retry(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_pipeline_job(\"retry_pipeline\", retry_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8928d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
